{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyCim7Vh_ass",
        "outputId": "ceae9824-fd6a-4ac9-fcd2-fc8c1f971a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.30.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2023.5.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: accelerate in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (0.4.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: requests in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->accelerate) (2.30.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->accelerate) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->accelerate) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hasha\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "     ---------------------------------------- 0.0/647.5 kB ? eta -:--:--\n",
            "      --------------------------------------- 10.2/647.5 kB ? eta -:--:--\n",
            "     -- ---------------------------------- 41.0/647.5 kB 495.5 kB/s eta 0:00:02\n",
            "     ------- ------------------------------ 133.1/647.5 kB 1.1 MB/s eta 0:00:01\n",
            "     ---------------- --------------------- 276.5/647.5 kB 1.7 MB/s eta 0:00:01\n",
            "     ----------------------- -------------- 399.4/647.5 kB 1.9 MB/s eta 0:00:01\n",
            "     -------------------------------- ----- 553.0/647.5 kB 2.2 MB/s eta 0:00:01\n",
            "     -------------------------------------  645.1/647.5 kB 2.3 MB/s eta 0:00:01\n",
            "     -------------------------------------- 647.5/647.5 kB 2.0 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py): started\n",
            "  Building wheel for annoy (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for annoy\n",
            "Failed to build annoy\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [14 lines of output]\n",
            "      C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\setuptools\\installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
            "        warnings.warn(\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-311\n",
            "      creating build\\lib.win-amd64-cpython-311\\annoy\n",
            "      copying annoy\\__init__.py -> build\\lib.win-amd64-cpython-311\\annoy\n",
            "      copying annoy\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\annoy\n",
            "      copying annoy\\py.typed -> build\\lib.win-amd64-cpython-311\\annoy\n",
            "      running build_ext\n",
            "      building 'annoy.annoylib' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for annoy\n",
            "ERROR: Could not build wheels for annoy, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "#Muhammad Hasham Ul Haq\n",
        "#i200752@nu.edu.pk\n",
        "%pip install transformers\n",
        "%pip install accelerate\n",
        "%pip install annoy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jn9jG0BE7sa",
        "outputId": "c465dd32-f5e2-41a9-a93f-0b0ac6131fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset:\n",
            "                                            Urdu Story  Split_ID\n",
            "228  میں نے کل بھی کہا تھا اور آج پھر کہہ رہا ہوں ک...       228\n",
            "208  اف کیا بتاؤں تمہیں گھر کی شفٹنگ کتنا مشکل کام ...       208\n",
            "96   میرے ابّو ریلوے میں ملازم تھے۔ اچانک زندگی کی ...        96\n",
            "167  جب میری شادی منیر سے ہوئی ، ان دنوں وہ ایک سرک...       167\n",
            "84   کوئی چوتھی بار آ کر فاطمہ نے کھڑکی سے باہر جھا...        84\n",
            "\n",
            "Validation Dataset:\n",
            "                                            Urdu Story  Split_ID\n",
            "24   ہم کھاتے پیتے لوگ تھے۔ مجھے ملازمت کی ضرورت نہ...        24\n",
            "6    چالیس دن کی تھی جب میری زندگی کی عجب کہانی کی ...         6\n",
            "153  دلہن رخصت ہو کر بخیر و خوبی کار تک رسائی حاصل ...       153\n",
            "211  موسم سرما نے پاؤں پاؤں چلنا شروع کیا تھا۔ سورج...       211\n",
            "198  میرے دو چھوٹے بھائی تھے لیکن والدین ان سے زیاد...       198\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('urdu_stories.csv')\n",
        "\n",
        "# Create a new column for split ID (you can adjust this based on your requirements)\n",
        "df['Split_ID'] = range(len(df))\n",
        "\n",
        "# Drop the 'Title' column\n",
        "df = df[['Urdu Story', 'Split_ID']]  # Add 'Split_ID' to the relevant columns\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save the preprocessed datasets\n",
        "train_df.to_csv('train_dataset.csv', index=False)\n",
        "valid_df.to_csv('valid_dataset.csv', index=False)\n",
        "\n",
        "# Display a few rows of the training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Display a few rows of the validation dataset\n",
        "print(\"\\nValidation Dataset:\")\n",
        "print(valid_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAFz4Kf8oPKN",
        "outputId": "56acf9e1-9430-437a-a637-e2958d98496a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\hasha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "                                                          \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./distilbert-finetuned\\\\tokenizer_config.json',\n",
              " './distilbert-finetuned\\\\special_tokens_map.json',\n",
              " './distilbert-finetuned\\\\vocab.txt',\n",
              " './distilbert-finetuned\\\\added_tokens.json')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "class UrduDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        # Preprocess label to handle non-integer values\n",
        "        try:\n",
        "            label = int(self.labels[idx])\n",
        "        except ValueError:\n",
        "            label = 0  # Set a default label or handle the case based on your specific requirements\n",
        "\n",
        "        encoding = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
        "\n",
        "\n",
        "train_dataset = UrduDataset(train_df['Urdu Story'].tolist(), train_df['Urdu Story'].tolist(), tokenizer)\n",
        "valid_dataset = UrduDataset(valid_df['Urdu Story'].tolist(), valid_df['Urdu Story'].tolist(), tokenizer)\n",
        "\n",
        "# Load model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "\n",
        "# Define training arguments\n",
        "epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False):\n",
        "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./distilbert-finetuned\")\n",
        "tokenizer.save_pretrained(\"./distilbert-finetuned\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozb4wv2Gp6F0",
        "outputId": "0ba6be28-05e8-4028-dc95-8904bf315887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 0: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 1: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 2: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 3: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 4: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 5: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 6: 0\n",
            "Skipping training for split 6 as the dataset is empty.\n",
            "Dataset length for split 7: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 8: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length for split 9: 0\n",
            "Skipping training for split 9 as the dataset is empty.\n"
          ]
        }
      ],
      "source": [
        "total_splits = 10\n",
        "mini_lm_epochs = 3  # Set the number of training epochs\n",
        "mini_lm_models = []  # Initialize the list to store mini-LM models\n",
        "\n",
        "for split_id in range(total_splits):\n",
        "    # Create mini-LM model\n",
        "    mini_lm_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "    mini_lm_linear = torch.nn.Linear(mini_lm_model.config.hidden_size, tokenizer.vocab_size).to(device)\n",
        "    mini_lm_optimizer = optim.AdamW(list(mini_lm_model.parameters()) + list(mini_lm_linear.parameters()), lr=5e-5)\n",
        "\n",
        "    # Create mini-LM dataset for the current split\n",
        "    mini_lm_dataset = UrduDataset(train_df[train_df['Split_ID'] == split_id]['Urdu Story'].tolist(),\n",
        "                                  train_df[train_df['Split_ID'] == split_id]['Urdu Story'].tolist(), tokenizer)\n",
        "\n",
        "    print(f\"Dataset length for split {split_id}: {len(mini_lm_dataset)}\")  # Add this line to check the length of the dataset\n",
        "\n",
        "    # Skip training for empty datasets\n",
        "    if len(mini_lm_dataset) == 0:\n",
        "        print(f\"Skipping training for split {split_id} as the dataset is empty.\")\n",
        "        continue\n",
        "\n",
        "    # Mini-LM Training\n",
        "    for epoch in range(mini_lm_epochs):\n",
        "        mini_lm_model.train()\n",
        "        mini_lm_loader = DataLoader(mini_lm_dataset, batch_size=8, shuffle=True)\n",
        "        for batch in tqdm(mini_lm_loader, desc=f'Mini-LM Epoch {epoch + 1}/{mini_lm_epochs}', leave=False):\n",
        "            input_ids, attention_mask, _ = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            mini_lm_optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass through mini-LM model\n",
        "            outputs = mini_lm_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Extract last hidden states\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "            # Pass through linear layer to get logits\n",
        "            logits = mini_lm_linear(last_hidden_state)\n",
        "\n",
        "            # Flatten the logits and input_ids for cross-entropy loss\n",
        "            logits_flat = logits.view(-1, logits.size(-1))\n",
        "            input_ids_flat = input_ids.view(-1)\n",
        "\n",
        "            # Calculate cross-entropy loss\n",
        "            loss = F.cross_entropy(logits_flat, input_ids_flat)\n",
        "\n",
        "            # Backward pass and optimization step\n",
        "            loss.backward()\n",
        "            mini_lm_optimizer.step()\n",
        "\n",
        "    # Save the trained mini-LM\n",
        "    mini_lm_model.save_pretrained(f\"./mini_lm_split_{split_id}\")\n",
        "    torch.save(mini_lm_linear.state_dict(), f\"./mini_lm_split_linear_{split_id}\")\n",
        "    tokenizer.save_pretrained(f\"./mini_lm_split_{split_id}\")\n",
        "\n",
        "    # Append the trained mini-LM to the list\n",
        "    mini_lm_models.append((mini_lm_model, mini_lm_linear))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqFMub6rFxfV",
        "outputId": "b7345f8e-480f-44b3-f27d-a73963be2c63"
      },
      "outputs": [],
      "source": [
        "from annoy import AnnoyIndex\n",
        "import os\n",
        "\n",
        "# Directory to store the Annoy index files\n",
        "index_dir = \"annoy_index\"\n",
        "os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "# Loop over splits\n",
        "for split_id, (mini_lm_model, mini_lm_linear) in enumerate(mini_lm_models):\n",
        "    # Use mini-LM to obtain embeddings for the corresponding split\n",
        "    split_embeddings = []\n",
        "\n",
        "    # Create a new dataset for obtaining embeddings\n",
        "    split_texts = train_df[train_df['Split_ID'] == split_id]['Urdu Story'].tolist()\n",
        "\n",
        "    if len(split_texts) > 0:\n",
        "        split_dataset = UrduDataset(split_texts, [0] * len(split_texts), tokenizer)\n",
        "        split_loader = DataLoader(split_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mini_lm_model.eval()\n",
        "            mini_lm_linear.eval()\n",
        "            for batch in tqdm(split_loader, desc=f'Embedding Split {split_id}', leave=False):\n",
        "                input_ids, attention_mask, _ = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "\n",
        "                # Forward pass through mini-LM model\n",
        "                outputs = mini_lm_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "                # Extract last hidden states\n",
        "                last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "                # Pass through linear layer to get logits\n",
        "                logits = mini_lm_linear(last_hidden_state)\n",
        "\n",
        "                # Use mean pooling to obtain embeddings\n",
        "                embeddings = last_hidden_state.mean(dim=1)  # You can modify this based on your requirements\n",
        "                split_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "        # Check if split_embeddings is not empty before concatenating and storing\n",
        "        if split_embeddings:\n",
        "            # Save the split embeddings along with relevant information\n",
        "            split_embeddings = np.concatenate(split_embeddings, axis=0)\n",
        "\n",
        "            # Dimensionality of the embeddings\n",
        "            embedding_dim = split_embeddings.shape[1]\n",
        "\n",
        "            # Create an Annoy index with cosine similarity (metric=2)\n",
        "            annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
        "\n",
        "            # Add embeddings to the Annoy index\n",
        "            for i, embedding in enumerate(split_embeddings):\n",
        "                annoy_index.add_item(i, embedding)\n",
        "\n",
        "            # Build the Annoy index\n",
        "            annoy_index.build(10)  # Adjust the number of trees based on your requirements\n",
        "\n",
        "            # Save the Annoy index to disk\n",
        "            annoy_index.save(f\"{index_dir}/annoy_index_split_{split_id}.ann\")\n",
        "\n",
        "            # Store any additional information (e.g., split ID, story ID) in a separate file or database\n",
        "\n",
        "            print(f\"Annoy index for split {split_id} created and saved.\")\n",
        "        else:\n",
        "            print(f\"No samples found for split {split_id}. Skipping.\")\n",
        "    else:\n",
        "        print(f\"No samples found for split {split_id}. Skipping.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG4J7C8yxFZa",
        "outputId": "fa7da7ec-ad69-4c9d-da1e-5810083e8a40"
      },
      "outputs": [],
      "source": [
        "#Load Mini-LM and Annoy Index for the Corresponding Split\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from annoy import AnnoyIndex\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load mini-LM model\n",
        "mini_lm_model = DistilBertModel.from_pretrained('mini_lm_split_7')\n",
        "mini_lm_model.to(device)\n",
        "\n",
        "# Load mini-LM linear layer\n",
        "mini_lm_linear = torch.nn.Linear(mini_lm_model.config.hidden_size, tokenizer.vocab_size).to(device)\n",
        "mini_lm_linear.load_state_dict(torch.load('mini_lm_split_linear_7'))\n",
        "mini_lm_linear.eval()\n",
        "\n",
        "# Load Annoy index\n",
        "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
        "annoy_index.load('/content/annoy_index/annoy_index_split_7.ann')\n",
        "\n",
        "# Check the shape of the mini-LM model\n",
        "#print(f\"Mini-LM Model Shape: {mini_lm_model}\")\n",
        "#print(f\"Mini-LM Linear Layer Shape: {mini_lm_linear}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2amYhJHy3Gv"
      },
      "outputs": [],
      "source": [
        "def get_query_embeddings(query, mini_lm_model, mini_lm_linear, tokenizer, device):\n",
        "    with torch.no_grad():\n",
        "        mini_lm_model.eval()\n",
        "        mini_lm_linear.eval()\n",
        "\n",
        "        # Tokenize and encode the query\n",
        "        encoding = tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
        "        input_ids, attention_mask = encoding['input_ids'].to(device), encoding['attention_mask'].to(device)\n",
        "\n",
        "        # Forward pass through mini-LM model\n",
        "        outputs = mini_lm_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract last hidden states\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "        # Pass through linear layer to get logits\n",
        "        logits = mini_lm_linear(last_hidden_state)\n",
        "\n",
        "        # Use mean pooling to obtain embeddings\n",
        "        query_embeddings = last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "    print(f\"Shape of query_embeddings: {query_embeddings.shape}\")  # Add this line for debugging\n",
        "    return query_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF4z3gzUzHr5"
      },
      "outputs": [],
      "source": [
        "def query_vector_database(query_embeddings, annoy_index, top_n=5):\n",
        "    # Flatten the query_embeddings to make it 1D\n",
        "    query_embeddings_flat = query_embeddings.flatten()\n",
        "\n",
        "    # Get similar splits and distances\n",
        "    similar_splits, distances = annoy_index.get_nns_by_vector(query_embeddings_flat, top_n, include_distances=True)\n",
        "\n",
        "    # Return as a list of tuples\n",
        "    return list(zip(similar_splits, distances))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbXaaUJqzPyT",
        "outputId": "d8c734df-d7d8-417e-d5a7-cc8f187102c2"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "user_query = \"علی کون تھا؟\"\n",
        "query_embeddings = get_query_embeddings(user_query, mini_lm_model, mini_lm_linear, tokenizer, device)\n",
        "similar_splits = query_vector_database(query_embeddings, annoy_index, top_n=5)\n",
        "\n",
        "for split_index, distance in similar_splits:\n",
        "    urdu_text_of_similar_split = valid_df.iloc[split_index]['Urdu Story']\n",
        "    print(f\"Similar Split Index: {split_index}, Distance: {distance}, Urdu Text: {urdu_text_of_similar_split}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Evaluating Model\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model_and_annoy(valid_dataset, model, mini_lm_model, mini_lm_linear, tokenizer, annoy_index, top_n=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_queries = len(valid_dataset)\n",
        "\n",
        "    for idx in tqdm(range(total_queries), desc=\"Evaluating Model and Annoy Index\"):\n",
        "        query = valid_dataset['Urdu Story'].iloc[idx]\n",
        "        true_label = valid_dataset['Split_ID'].iloc[idx]\n",
        "\n",
        "        # Get query embeddings using the fine-tuned DistilBERT model\n",
        "        query_embeddings = get_query_embeddings(query, mini_lm_model, mini_lm_linear, tokenizer, device)\n",
        "\n",
        "        # Get similar splits using the Annoy Index\n",
        "        similar_splits = query_vector_database(query_embeddings, annoy_index, top_n=top_n)\n",
        "\n",
        "        # Check if the true label is among the top similar splits\n",
        "        if true_label in [split_index for split_index, _ in similar_splits]:\n",
        "            correct_predictions += 1\n",
        "        # Inside the loop of evaluate_model_and_annoy function\n",
        "        print(f\"True Label: {true_label}\")\n",
        "        print(f\"Top Similar Splits: {similar_splits}\")\n",
        "\n",
        "\n",
        "    accuracy = correct_predictions / total_queries\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Correct Predictions: {correct_predictions}/{total_queries}\")\n",
        "\n",
        "# Load the fine-tuned DistilBERT model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('./distilbert-finetuned').to(device)\n",
        "\n",
        "# Load mini-LM model\n",
        "mini_lm_model = DistilBertModel.from_pretrained('mini_lm_split_0').to(device)\n",
        "\n",
        "# Load mini-LM linear layer\n",
        "mini_lm_linear = torch.nn.Linear(mini_lm_model.config.hidden_size, tokenizer.vocab_size).to(device)\n",
        "mini_lm_linear.load_state_dict(torch.load('./mini_lm_split_linear_3'))\n",
        "mini_lm_linear.eval()\n",
        "\n",
        "# Load the Annoy Index\n",
        "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
        "annoy_index.load('./annoy_index/annoy_index_split_5.ann')  # Change the split index based on your saved Annoy Index\n",
        "\n",
        "# Evaluate the model and Annoy Index\n",
        "evaluate_model_and_annoy(valid_df, model, mini_lm_model, mini_lm_linear, tokenizer, annoy_index, top_n=5)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
